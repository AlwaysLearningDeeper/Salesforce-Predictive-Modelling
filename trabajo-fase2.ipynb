{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=Indice></a>\n",
    "# Índice\n",
    "1. [Análisis de datos](#Análisisdedatos)\n",
    " 1. [Análisis de la variable dependiente](#Análisisdelavariabledependiente)\n",
    " 2. [Análisis de las variables explicativas](#Análisisdelasvariablesexplicativas)\n",
    "2. [Entrenamiento de modelos](#Entrenamientodemodelos)\n",
    "    1. [Preparación de datos](#Preparacióndedatos)\n",
    "    2. [Experimentos](#Experimentos)\n",
    "    3. [Comparación de modelos](#Comparacióndemodelos)\n",
    "    4. [Optimización y combinación de modelos](#Optimizacion)\n",
    "3. [Predicción](#Predicción)\n",
    "4. [Anexo 1: Otros modelos considerados](#Anexo1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tpot import TPOTRegressor\n",
    "from mlens.ensemble import SuperLearner,BlendEnsemble\n",
    "from sklearn.metrics import make_scorer,precision_score,accuracy_score,recall_score\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet,HuberRegressor,SGDRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier, GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,median_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jairsan/Documents/dev/AlwaysLearningDeeper/Salesforce-Predictive-Modelling/venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Cargamos los datos de entrenamiento\n",
    "train_df = pd.read_csv(\"data/original/Dataset_Salesforce_Predictive_Modelling_TRAIN.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este reto es es desarollo de un modelo capaz de predecir el poder adquisitivo de un cliente a partir de una serie de variables disponibles para la entidad financiera. Contar con un modelo preciso que pueda llevar a cabo estars predicciones sin duda conllevaría muchas ventajas a la hora de recomendar productos al cliente que se adecuen a sus necesidades y capacidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=Análisisdedatos></a>\n",
    "# Análisis de datos\n",
    "[Volver al Indice](#Indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Veamos un ejemplo de los datos con los que contamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como se explica en el enunciado, contamos con 88 variables que usar para predecir el poder adquisitivo y la información de cerca de 363.000 clientes con los que entrenar nuestro modelo. Esto hace que contemos con un número importante de datos que nos puede ayudar a desarrollar un modelo que tenga un rendimiento adecuado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=Análisisdelavariabledependiente> </a>\n",
    "## Análisis de la variable dependiente \n",
    "Miremos con más detenimiento la variable a predecir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df[\"Poder_Adquisitivo\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df[\"Poder_Adquisitivo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % train_df[\"Poder_Adquisitivo\"].skew())\n",
    "print(\"Kurtosis: %f\" % train_df[\"Poder_Adquisitivo\"].kurt())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tal y como se puede observar en el gráfico, esta variable presenta valores extremos de skewness y kurtosis, que se manifiestan en la larga cola derecha que presenta. Esto quiere decir que existe algunos clientes que presentan valores extremadamente altos de poder adquisitivo que están muy lejos del resto. Será necesario tener en cuenta este hecho para evitar que los valores extremos puedan afectar negativamente a nuestro sistema.\n",
    "\n",
    "Veamos de manera más detallada aquellos valores que tradicionalmente no se considerarían outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = train_df[\"Poder_Adquisitivo\"].quantile(0.25)\n",
    "q3 = train_df[\"Poder_Adquisitivo\"].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "fence_low  = q1 - 1.5 * iqr\n",
    "fence_high = q3 + 1.5 * iqr\n",
    "\n",
    "train_df_no_outliers = train_df.loc[(train_df[\"Poder_Adquisitivo\"] > fence_low) & (train_df[\"Poder_Adquisitivo\"] < fence_high)]\n",
    "sns.distplot(train_df_no_outliers[\"Poder_Adquisitivo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % train_df_no_outliers[\"Poder_Adquisitivo\"].skew())\n",
    "print(\"Kurtosis: %f\" % train_df_no_outliers[\"Poder_Adquisitivo\"].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Porcentaje de datos eliminados:\")\n",
    "print((len(train_df.index)-len(train_df_no_outliers.index))/len(train_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminando de esta manera los valores extremos, que representan alrededor del 5% de los datos totales, la distribución ahora presenta unos valores de skewness y kurtosis mucho más aceptables que permitan el entrenamiento de un modelo.\n",
    "\n",
    "Evidentemente esta eliminación se va a realizar únicamente a la hora de entrenar, nunca a la hora de evaluar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=Análisisdelasvariablesexplicativas></a>\n",
    "## Análisis de las variables explicativas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las variables explicativas, sabemos que contamos con algunas que son de tipo categórico en vez de númerico. Empezemos explorando estas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df_no_outliers[\"Socio_Demo_01\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como Socio_Demo_01 que cuenta que muchos valores que solo aparecen un número muy bajo de veces. A la hora de transformar para su uso, es probable que la inclusión de los 921 valores posibles no aporte información discriminativa y solo sirva para aumentar el número de dimensiones. Una primera aproximación que mantenga un equilibrio entre complejidad y utilidad puede ser usar solo un número de estos valores, aquellos que aparezcan un mayor número de veces, y condensar el resto en una categoría \"Other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_socio_01 = train_df_no_outliers[\"Socio_Demo_01\"].value_counts()[:10]\n",
    "topk_socio_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socio_01_keys = list(topk_socio_01.keys())\n",
    "condition_array = [False] * len(train_df_no_outliers[\"Socio_Demo_01\"])\n",
    "for i in range(len(condition_array)):\n",
    "    condition_array[i] = str(train_df_no_outliers[\"Socio_Demo_01\"].iloc[i]) in socio_01_keys\n",
    "\n",
    "sns.violinplot(x=train_df_no_outliers[\"Socio_Demo_01\"].loc[condition_array],y=train_df_no_outliers[\"Poder_Adquisitivo\"].loc[condition_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante ver como algunos valores como 09992 parecen concentrar la mayor parte de clientes en valores diferentes del resto, lo cual puede aportar información importante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_no_outliers[\"Socio_Demo_02\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=train_df_no_outliers[\"Socio_Demo_02\"],y=train_df_no_outliers[\"Poder_Adquisitivo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el resto de variables socio demográficas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df_no_outliers[\"Socio_Demo_03\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=train_df_no_outliers[\"Socio_Demo_03\"], y=train_df_no_outliers[\"Poder_Adquisitivo\"],kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(train_df_no_outliers[\"Socio_Demo_04\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=train_df_no_outliers[\"Socio_Demo_04\"], y=train_df_no_outliers[\"Poder_Adquisitivo\"],kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_no_outliers[\"Socio_Demo_04\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuevo, observamos que una serie pequeña de valores concentra la gran mayoría de ocurrencias. Veamos si existe una relación aparente entre cada valor de esta variable y el poder adquisitivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "sns.boxplot(x=train_df_no_outliers[\"Socio_Demo_04\"], y=train_df_no_outliers[\"Poder_Adquisitivo\"],ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos valores muestran distribuciones que son diferentes a las demás, pero aparecen un número casi insignificante de veces respecto a la totalidad de los datos, por lo que no consideremos que sea necesario complicar el modelo con esos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df_no_outliers[\"Socio_Demo_05\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=train_df_no_outliers[\"Socio_Demo_05\"], y=train_df_no_outliers[\"Poder_Adquisitivo\"],kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora la correlación de las variables explicativas con el poder adquisitivo, medida mediante el coeficiente de Pearson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df_no_outliers.corr(method='pearson').iloc[-1].sort_values(ascending=False,axis=0)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos como algunas variables presentan coeficientes de correlación medios, lo que indica que serán útiles para los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=Entrenamientodemodelos></a>\n",
    "# Entrenamiento de modelos\n",
    "[Volver al Indice](#Indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=Preparacióndedatos></a>\n",
    "## Preparación de datos\n",
    "Con todo lo visto anteriormente estamos listos para preparar los datos para la experimentación. Vamos a definir una función que realize este pre-procesado de los datos. \n",
    "\n",
    "La transformación básica consiste en convertir las variables categóricas a valores one-hot, y eliminar la columna ID_Customer que no es más que un identificador del cliente.\n",
    "\n",
    "\n",
    "Con processing_type = 1 se puede indicar además que queremos filtrar los outliers (pero únicamente en tiempo de entrenamiento)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df,processing_type,train = True):\n",
    "    if processing_type == 1:\n",
    "        return process_df_1(df,train)\n",
    "    else:\n",
    "        return process_df_1(df,False)\n",
    "    \n",
    "def process_df_1(df,train = True):\n",
    "    df = df.drop(labels=[\"ID_Customer\"],axis=1)\n",
    "    \n",
    "    if train:\n",
    "        # Eliminamos los outliers solo en el caso de que estemos entrenando\n",
    "        q1 = df[\"Poder_Adquisitivo\"].quantile(0.25)\n",
    "        q3 = df[\"Poder_Adquisitivo\"].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        fence_low  = q1 - 1.5 * iqr\n",
    "        fence_high = q3 + 1.5 * iqr\n",
    "\n",
    "        df = df.loc[(df[\"Poder_Adquisitivo\"] > fence_low) & (df[\"Poder_Adquisitivo\"] < fence_high)]\n",
    "    \n",
    "        \n",
    "    \n",
    "    # Convertimos las variables a one-hot\n",
    "\n",
    "    # Socio_Demo_01\n",
    "    \n",
    "    topk_socio_01 = df[\"Socio_Demo_01\"].value_counts()[:10]\n",
    "    socio_01_keys = list(topk_socio_01.keys())\n",
    "    \n",
    "    for key in socio_01_keys:\n",
    "        on = df[\"Socio_Demo_01\"] == key\n",
    "        df.insert(loc=len(df.columns), column=\"Socio_Demo_01_\"+str(key), value=on.astype(int))\n",
    "    \n",
    "    # El resto lso agrupamos en 'Other'\n",
    "    condition_array = [False] * len(df[\"Socio_Demo_01\"])\n",
    "    for i in range(len(condition_array)):\n",
    "        condition_array[i] = str(df[\"Socio_Demo_01\"].iloc[i]) not in socio_01_keys\n",
    "    df.insert(loc=len(df.columns), column=\"Socio_Demo_01_Other\", value=condition_array)\n",
    "    df[\"Socio_Demo_01_Other\"] = df[\"Socio_Demo_01_Other\"].astype(int)\n",
    "    \n",
    "    df = df.drop(axis=1, columns=[\"Socio_Demo_01\"])\n",
    "    \n",
    "    # Socio_Demo_02\n",
    "    c1=df[\"Socio_Demo_02\"] == 1\n",
    "    c2=df[\"Socio_Demo_02\"] == 2\n",
    "    df.insert(loc=len(df.columns), column=\"Socio_Demo_02_01\", value=c1.astype(int))\n",
    "    df.insert(loc=len(df.columns), column=\"Socio_Demo_02_02\", value=c2.astype(int))\n",
    "    \n",
    "    df = df.drop(axis=1, columns=[\"Socio_Demo_02\"])\n",
    "\n",
    "    \n",
    "    # Convertimos todas las columnas Ind_prod a one-hot\n",
    "    \n",
    "    for i in range(1,25):\n",
    "        column_name = \"Ind_Prod_\" + str(i).zfill(2)\n",
    "        c0=df[column_name] == 0\n",
    "        c1=df[column_name] == 1\n",
    "        c2=df[column_name] == 2\n",
    "        \n",
    "        df.insert(loc=len(df.columns), column=column_name + \"_00\", value=c0.astype(int))\n",
    "        df.insert(loc=len(df.columns), column=column_name + \"_01\", value=c1.astype(int))\n",
    "        df.insert(loc=len(df.columns), column=column_name + \"_02\", value=c2.astype(int))\n",
    "    \n",
    "        df = df.drop(axis=1, columns=[column_name])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vamos a barjar los datos, y a continuación vamos a realizar una partición de los datos para realizar **validación cruzada** en 5 bloques, con el objetivo de realizar un análisis de resultados de los diferentes modelos de manera correcta y fiable.\n",
    "\n",
    "Este método divide los datos en los conjustos de entrenamiento y test para cada partición, y los almacena en la variable splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 4\n",
    "K = 5\n",
    "\n",
    "shuffled_data = train_df.sample(frac=1,replace=False,random_state=SEED)\n",
    "\n",
    "kf = KFold(n_splits=K)\n",
    "kf.get_n_splits(shuffled_data)\n",
    "\n",
    "def get_splits(kf,processing_type):\n",
    "    splits=[]\n",
    "    for train_index, test_index in kf.split(shuffled_data):\n",
    "        train_data = shuffled_data.loc[train_index]\n",
    "        test_data = shuffled_data.loc[test_index]\n",
    "\n",
    "        train_data_proc = process_df(train_data,processing_type,train=True)\n",
    "        test_data_proc = process_df(test_data,processing_type,train=False)\n",
    "\n",
    "\n",
    "        splits.append((train_data_proc,test_data_proc))\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado un modelo y los splits que hemos realizado a los datos, a continuación definimos una función que, para cada partición de entrenamiento y test, entrene un modelo con los datos de entrenamiento correspondiente y calcule métricas sobre el conjunto de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo que se refiere a métricas, en primer lugar hemos escogido la **ráiz del error cuadrático medio (RMSE)**, una de las métricas más comunes para evaluar modelos de regresión. El problema que tiene esta métrica es que penaliza de manera desmedida fallos grandes. Resulta mucho más interesante medir el funcionamiento del algoritmo con una métrica que refleje mejor su funcionamiento con la mayoría de clientes, cosa que consideramos más útil a la hora de decidirnos por un modelo u otro. Por eso medimos los resultados con respecto a la **media del error absoluto (MAE)** y a la **mediana del error (MAD).** Esta última métrica no es susceptible a outliers y nos permite conocer mejor cual es el comportamiento \"medio\" de cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model,splits,skcompat=False,scaler=None):\n",
    "    rmse = []\n",
    "    mae = []\n",
    "    mad = []\n",
    "    # Para cada iteración de validación cruzada\n",
    "    for s in range(len(splits)):\n",
    "        train_data_proc,test_data_proc = splits[s]\n",
    "        \n",
    "        # Obtenemos los datos de entrenamiento\n",
    "        x_train = train_data_proc.drop(labels=[\"Poder_Adquisitivo\"],axis=1).as_matrix()\n",
    "        y_train = train_data_proc[\"Poder_Adquisitivo\"].as_matrix()\n",
    "\n",
    "        # Obtenemos los datos de test\n",
    "        x_test = test_data_proc.drop(labels=[\"Poder_Adquisitivo\"], axis=1).as_matrix()\n",
    "        y_test = test_data_proc[\"Poder_Adquisitivo\"].as_matrix()\n",
    "        \n",
    "        # Damos la posiblidad de usar un scaler\n",
    "        if scaler is not None:\n",
    "            x_train = scaler.fit_transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "           \n",
    "        # Caso base, entrenamos un modelo y obtenemos las predicciones\n",
    "        if not skcompat:\n",
    "            model.fit(X=x_train,y=y_train)\n",
    "            \n",
    "            # For compatibility with XGBoost\n",
    "            # yhat = model.predict(X=x_test)\n",
    "            yhat = model.predict(x_test)\n",
    "        \n",
    "        # En el caso de que sea un objeto skcompat, las llamadas a los métodos son ligeramente diferentes.\n",
    "        else:\n",
    "            model.fit(x=x_train,y=y_train,steps=STEPS)\n",
    "            yhat = model.predict(x=x_test)['scores']\n",
    "        \n",
    "        # Calculamos métricas \n",
    "        rmse.append(math.sqrt(mean_squared_error(y_true=y_test, y_pred=yhat)))\n",
    "        mae.append(mean_absolute_error(y_true=y_test,y_pred=yhat))\n",
    "        mad.append(median_absolute_error(y_true=y_test,y_pred=yhat))\n",
    "        \n",
    "    return (rmse,mae,mad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, nos definimos un método para guardar los resultados para su posterior visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'modelo':[], 'rmse':[],'mae':[],'mad':[]}\n",
    "\n",
    "def record_scores(name,rmse,mae,mad):\n",
    "    scores['modelo'].append(name)\n",
    "    scores['rmse'].append(rmse)\n",
    "    scores['mae'].append(mae)\n",
    "    scores['mad'].append(mad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=Experimentos></a>\n",
    "## Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a exponer una serie de resultados que hemos obtenido evaluando diferentes modelos. Cabe destacar que la cantidad de experimentos realizada es mucho mayor de la mostrada aquí, pero se ha llevado a cabo una selección de aquellos que, a nuestro juicio, son más interesantes. En caso contrario este documento hubiera sido todavía más largo de lo que ya es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar probaremos diferentes modelos tras aplicar el preproceso en el que eliminamos **outliers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = []\n",
    "splits = get_splits(kf,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente probamos diferentes modelos de regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('Linear Regresion',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(max_iter=200,hidden_layer_sizes=(50,50,50,50),early_stopping=True,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('ANN50*4',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos ahora diferentes modelos basados en árboles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(min_samples_split=200, min_samples_leaf=50, random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('RegressionTree',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=23, max_features='log2',min_samples_leaf=5,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('RF_d23',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=23, max_features='auto',min_samples_leaf=5,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('RF_d23_FULL',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El permitir que se hagan splits sobre todas las features incrementa sustancialmente el tiempo de entrenamiento, pero hace que este modelo supere al rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos ahora con la técnica de Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(random_state=SEED,min_samples_leaf=5,max_depth=5)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('Gradient Boost_d5',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(random_state=SEED,min_samples_leaf=5,max_depth=7)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('Gradient Boost_d7',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuevo, llegados a este punto los modelos empiezan a requerar un tiempo de entrenamiento elevado a medida que se aumenta la profundidad máxima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una comparación importante a realizar es comprobar si nuestra idea inicial de eliminar outliers ha dado sus frutos. Probamos ahora a entrenar una serie de modelos ***sin eliminar outliers***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = []\n",
    "splits = get_splits(kf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('raw Linear Regression',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MLPRegressor(max_iter=200,hidden_layer_sizes=(50,50,50,50),early_stopping=True,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('raw ANN50*4',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(random_state=SEED,min_samples_leaf=5,max_depth=5)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('raw Gradient Boost_d5',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(random_state=SEED,min_samples_leaf=5,max_depth=7)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('raw Gradient Boost_d7',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))\n",
    "\n",
    "\"\"\"\n",
    "RMSE: 19659.521910\n",
    "MAE: 4686.435566\n",
    "MAD: 2390.403421\n",
    "\"\"\"\n",
    "\"\"\" Y un  montón de tiempo\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params =  {'max_depth': 13, 'eta': 0.1, 'silent': 1, 'objective': 'reg:linear', 'subsample':0.8,'eval_metric':'rmse','nthread':4,'seed':SEED}\n",
    "model = XGBoostModel(params,100)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('raw_XGB_d13_eta0.1_n100_sbspl',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=23, max_features='auto',min_samples_leaf=5,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('raw RF_d23_FULL',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgb.XGBRegressor(max_depth=13, learning_rate=0.1, alpha=0,n_estimators=30, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=0.8,colsample_bytree=1.0,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('raw_XGB_d13_eta0.1_n100_sbspl',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgb.XGBRegressor(max_depth=13, learning_rate=0.1, alpha=0,n_estimators=50, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=0.8,colsample_bytree=1.0,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('raw_XGB_d13_eta0.1_n100_sbspl',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorprendentemente, si bien para el resto de modelos si que apreciamos que el filtrado de outliers supone sustanciales ganancias en términos de MAE y MAD, en el caso del Random Forest la pequeña diferencia en MAD se ve compensado por una reducción muy importante en RMSE y MAE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=Comparacióndemodelos></a>\n",
    "## Comparación de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(data=scores)\n",
    "\n",
    "scores_df = scores_df.set_index('modelo')\n",
    "\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_df = scores_df.sort_values(by=\"mad\")\n",
    "scores_df.plot(kind='bar',y='mad',colormap='Blues_r',figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_df = scores_df.sort_values(by=\"mae\")\n",
    "scores_df.plot(kind='bar',y='mae',colormap='Oranges_r',figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_df = scores_df.sort_values(by=\"rmse\")\n",
    "scores_df.plot(kind='bar',y='rmse',colormap='Greens_r',figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta todo esto, finalmente nos hemos decidido por el modelo **raw RF_d23_n100_FULL**. Inicialmente, y con la mayoría de modelos, nuestra idea inicial de eliminar outliers había obtenido mejoras muy grandes en MAE y MAD que, a nuestro juicio, compensaban de sobra pequeñas pérdidas en RMSE ya que mejoraban el conjunto de las predicciones. Sin embargo, los modelos de Random Forest profundos han demostrado que son capaces de reducir de enorme manera los grandes errores cometidos en algunos clientes puntuales, a cambio de una pequeña pérdida de MAD. Si bien el rendimiento se reducirá muy ligeramente en general (en realidad se reduce de manera ínfima, si nos fijamos en el MAD, significa que el error mediano empeora sólo en 30€), esto nos va a permitir cometer fallos mucho más pequeños en clientes atípicos, ya que reducimos en más de 4000€ el RMSE, por lo que finalmente hemos optado por este modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=Optimizacion></a>\n",
    "## Optimización y combinación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De todos estos experimentos, se puede observar como los modelos más prometedores son el RandomForest y el GradientBoosting implementado sobre XGBoost. Ahora lo que nos gustaría hacer es encontrar los mejores hiperparámetros para estos problemas. Dado un cierto modelo, se puede entender el proceso de encontrar los **hiperparámetros óptimos** como un proceso de optimización de una **función**, que devuelve las métricas de validación cruzada del modelo, y cuyos **argumentos** son los hiperparámetros del modelo. Esta función es difícil de optimizar ya que:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Es una función **muy costosa** de evaluar, tenemos que entrenar por completo un modelo cada vez que queramos extraer un punto.\n",
    "* Tiene un **espacio grande** de posibles parámetros/argumentos.\n",
    "* **Desconocemos la estructura** de este espacio y las **interacciones entre los diferentes argumentos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **Optimización Bayesiana** nos ofrece una solución a este problema, ya que es una técnica pensada para optimizar funciones de tipo **caja-negra** como la que tenemos entre manos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train_df = process_df(shuffled_data,0,train = True)\n",
    "\n",
    "X_train = f_train_df.drop(labels=[\"Poder_Adquisitivo\"],axis=1).as_matrix()\n",
    "y_train = f_train_df[\"Poder_Adquisitivo\"].as_matrix()\n",
    "\n",
    "xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                 alpha):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=100, nfold=5,\n",
    "             seed=random_state,\n",
    "             callbacks=[xgb.callback.early_stop(5)])\n",
    "\n",
    "    return -cv_result['test-mae-mean'].values[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_state = SEED\n",
    "num_iter = 50\n",
    "init_points = 25\n",
    "params = {\n",
    "    'eta': 0.1,\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mae',\n",
    "    'verbose_eval': True,\n",
    "    'seed': random_state\n",
    "}\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 20),\n",
    "                                            'colsample_bytree': (0.1, 1),\n",
    "                                            'max_depth': (5, 20),\n",
    "                                            'subsample': (0.5, 1),\n",
    "                                            'gamma': (0, 15),\n",
    "                                            'alpha': (0, 15),\n",
    "                                            })\n",
    "\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "\n",
    "print(xgbBO.res['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train_df = process_df(shuffled_data,0,train = True)\n",
    "\n",
    "X_train = f_train_df.drop(labels=[\"Poder_Adquisitivo\"],axis=1).as_matrix()\n",
    "y_train = f_train_df[\"Poder_Adquisitivo\"].as_matrix()\n",
    "\n",
    "def model_evaluate(n_estimators,\n",
    "                 max_depth,\n",
    "                max_features,\n",
    "                 min_samples_leaf,\n",
    "                  ):\n",
    "\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    min_samples_leaf = int(min_samples_leaf)\n",
    "    max_features = max(min(max_features, 1), 0.1)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                 max_depth=max_depth,\n",
    "                max_features=max_features,\n",
    "                 min_samples_leaf=min_samples_leaf,\n",
    "                                 n_jobs=4,random_state=SEED)\n",
    "\n",
    "    cv_result = cross_val_score(model,X_train,y_train,cv=kf,scoring=\"neg_median_absolute_error\")\n",
    "\n",
    "    return np.mean(cv_result)\n",
    "\n",
    "\n",
    "num_iter = 50\n",
    "init_points = 10\n",
    "\n",
    "\n",
    "modelBO_2 = BayesianOptimization(model_evaluate, {'n_estimators': (30, 100),\n",
    "                                            'max_depth': (17, 27),\n",
    "                                            'min_samples_leaf': (5, 200),\n",
    "                                                'max_features' :(0.5,1),\n",
    "                                            })\n",
    "\n",
    "modelBO_2.maximize(init_points=init_points, n_iter=num_iter)\n",
    "\n",
    "print(modelBO_2.res['max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado del proceso de optimización hemos obtenido los siguientes parámetros para los dos modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgb.XGBRegressor(max_depth=20, learning_rate=0.1, alpha=0,n_estimators=30, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=1.0,colsample_bytree=1.0,min_child_weight=20.0,random_state=SEED )\n",
    "\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('opt_xgb_Boost',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=23, max_features='auto',min_samples_leaf=50,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('opt_raw RF_d23_FULL',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de escoger el mejor modelo, una primera aproximación consiste en escoger el modelo que haya obtenido mejores resultados. Otra opción más avanzada consiste en hacer una **combinación de modelos**. Veamos que sucedería si realizaramos una combinación de modelos utilizando el mismo modelo pero cambiando la semilla aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stk_train_df = process_df(shuffled_data.iloc[:int(shuffled_data.shape[0]*0.8)].copy(),0)\n",
    "stk_test_df = process_df(shuffled_data.iloc[int(shuffled_data.shape[0]*0.8):].copy(),0)\n",
    "\n",
    "\n",
    "X_train = stk_train_df.drop(labels=[\"Poder_Adquisitivo\"],axis=1).as_matrix()\n",
    "y_train = stk_train_df[\"Poder_Adquisitivo\"].as_matrix()\n",
    "\n",
    "X_test = stk_test_df.drop(labels=[\"Poder_Adquisitivo\"],axis=1).as_matrix()\n",
    "y_test = stk_test_df[\"Poder_Adquisitivo\"].as_matrix()\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(max_depth=20, learning_rate=0.1, alpha=0,n_estimators=30, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=1.0,colsample_bytree=1.0,min_child_weight=20.0,random_state=SEED)\n",
    "\n",
    "model_rf = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=23,\n",
    "                                 max_features='auto',min_samples_leaf=50,random_state=SEED)\n",
    "\n",
    "model_xgb.fit(X_train,y_train)\n",
    "residuals_xgb = y_test - model_xgb.predict(X_test)\n",
    "\n",
    "model_rf.fit(X_train,y_train)\n",
    "residuals_rf = y_test - model_rf.predict(X_test)\n",
    "\n",
    "\n",
    "model_xgb2 = xgb.XGBRegressor(max_depth=20, learning_rate=0.1, alpha=0,n_estimators=30, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=1.0,colsample_bytree=1.0,min_child_weight=20.0,random_state=33)\n",
    "\n",
    "model_rf2 = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=23,\n",
    "                                 max_features='auto',min_samples_leaf=50,random_state=33)\n",
    "\n",
    "model_xgb2.fit(X_train,y_train)\n",
    "residuals_xgb2 = y_test - model_xgb2.predict(X_test)\n",
    "\n",
    "model_rf2.fit(X_train,y_train)\n",
    "residuals_rf2 = y_test - model_rf2.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiemos la correlación que existe entre los errores que cometen los 4 modelos, 2 Random Forest y dos Gradient Boosts entrenados con XGBoost, entrenados con diferentes semillas, según la p de Pearson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'residuals_rf':residuals_rf,'residuals_xgb':residuals_xgb,'residuals_rf2':residuals_rf2,\n",
    "                        'residuals_xgb2':residuals_xgb2})\n",
    "corr = df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f47a8d60518>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47a8afbb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr,mask=mask,annot=True,vmin=0.9, vmax=1.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como los errores que comete un tipo de modelo están completamente correlacionados, independientemente de que se haya variado la semilla aleatoria. Por tanto, esto quiere decir que los modelos **cometen los mismos errores**. Hacer una combinación de modelos entrenados con distinta semilla, por tanto, no parece añadir muchas ventajas.\n",
    "\n",
    "En cambio, los errores comeditos por los modelos xgb y los Random Forest, si bien están fuertemente correlacionados, no son exactamente iguales. Los diferentes modelos no se equivoca, ni en el mismo sitio, ni de la misma manera. Combinar dos modelos diferentes, por tanto, nos aporta **diversidad**, y puede ayudarnos a mejorar los resultados ya que un modelo puede compensar fallos puntuales del otro para ciertos clientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedEnsemble:\n",
    "    def __init__(self,models,weights):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        for model in self.models:\n",
    "            model.fit(X,y)\n",
    "            \n",
    "    def predict(self,X):\n",
    "        y = np.zeros((X.shape[0]))\n",
    "        for i in range(len(self.models)):\n",
    "            y += (self.models[i].predict(X) * self.weights[i])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ensemble = WeightedEnsemble([xgb.XGBRegressor(max_depth=15, learning_rate=0.1, alpha=10,n_estimators=30, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=1.0,colsample_bytree=1.0,min_child_weight=1.0,random_state=SEED),\n",
    "                            RandomForestRegressor(n_estimators=50,\n",
    "                                       criterion='mse', max_depth=23, max_features='auto',n_jobs=4,\n",
    "                                       min_samples_leaf=5,random_state=SEED)],\n",
    "                            [0.5,0.5])\n",
    "\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(ensemble,splits)\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('beta_ensemble',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ensemble = WeightedEnsemble([xgb.XGBRegressor(max_depth=20, learning_rate=0.1, alpha=0,n_estimators=30, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=1.0,colsample_bytree=1.0,min_child_weight=20.0,random_state=SEED ),\n",
    "                            RandomForestRegressor(n_estimators=50,\n",
    "                                       criterion='mse', max_depth=23, max_features='auto',n_jobs=4,\n",
    "                                       min_samples_leaf=5,random_state=SEED)],\n",
    "                            [0.5,0.5])\n",
    "\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(ensemble,splits)\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('beta_ensemble',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=Predicción></a>\n",
    "# Predicción\n",
    "[Volver al Indice](#Indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splits = []\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=23, max_features='auto',min_samples_leaf=5,random_state=SEED)\n",
    "\n",
    "# Entrenamos con todos los datos\n",
    "f_train_df = process_df(train_df,0,train = True)\n",
    "\n",
    "x_train = f_train_df.drop(labels=[\"Poder_Adquisitivo\"],axis=1).as_matrix()\n",
    "y_train = f_train_df[\"Poder_Adquisitivo\"].as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X=x_train,y=y_train)\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(\"data/original/Dataset_Salesforce_Predictive_Modelling_TEST.txt\")\n",
    "ids = test_df[\"ID_Customer\"].copy()\n",
    "test_df =  process_df(test_df,0,train = False)\n",
    "\n",
    "x_test = test_df.as_matrix()\n",
    "\n",
    "# Estiamamos el poder adquisitivo\n",
    "y = model.predict(X=x_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo nos falta escribir estos resultados en disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(np.stack((ids, y), axis=1, out=None), columns=['ID_Customer', 'PA_Est']).set_index('ID_Customer')\n",
    "out.to_csv('Test_Mission.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=Anexo1></a>\n",
    "# Anexo 1: Otros modelos considerados\n",
    "[Volver al Indice](#Indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Lasso(random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('Lasso',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('Ridge',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElasticNet(random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('ElasticNet',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los modelos lineales parecen obtener resultados comparables. Probamos ahora con redes neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(max_iter=200,hidden_layer_sizes=(25,25,25),early_stopping=True,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(max_iter=200,hidden_layer_sizes=(50,50,50),early_stopping=True,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(max_iter=200,hidden_layer_sizes=(25,25,25,25),early_stopping=True,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(max_iter=200,hidden_layer_sizes=(25,25,25,25,25),early_stopping=True,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=5, max_features='log2',min_samples_leaf=5,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=7, max_features='log2',min_samples_leaf=5,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como la profundidad máxima afecta de manera importante al rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=17, max_features='log2',min_samples_leaf=5,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('RF_d17',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(random_state=SEED,min_samples_leaf=5)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=23, max_features='auto',min_samples_leaf=5,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('raw RF_d23_n100_FULL',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=24, max_features='auto',min_samples_leaf=5,random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('raw RF_d24_FULL',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(random_state=SEED)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('raw Gradient Boost_d3',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params =  {'max_depth': 7, 'eta': 0.1, 'silent': 1, 'objective': 'reg:linear', 'eval_metric':'rmse','nthread':3, 'seed':SEED}\n",
    "model = XGBoostModel(params,100)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('raw_XGB_d7_eta0.1_n100',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params =  {'max_depth': 9, 'eta': 0.1, 'silent': 1, 'objective': 'reg:linear', 'eval_metric':'rmse','nthread':3,'seed':SEED}\n",
    "model = XGBoostModel(params,100)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('raw_XGB_d9_eta0.1_n100',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params =  {'max_depth': 13, 'eta': 0.1, 'silent': 1, 'objective': 'reg:linear', 'eval_metric':'rmse','nthread':3,'seed':SEED}\n",
    "model = XGBoostModel(params,100)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('raw_XGB_d13_eta0.1_n100',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params =  {'max_depth': 13, 'eta': 0.1, 'silent': 1, 'objective': 'reg:linear', 'subsample':0.8,'eval_metric':'rmse','nthread':4,'seed':SEED}\n",
    "model = XGBoostModel(params,100)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('raw_XGB_d13_eta0.1_n100_sbspl',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train_df = process_df(shuffled_data,0,train = True)\n",
    "\n",
    "X_train = f_train_df.drop(labels=[\"Poder_Adquisitivo\"],axis=1).as_matrix()\n",
    "y_train = f_train_df[\"Poder_Adquisitivo\"].as_matrix()\n",
    "\n",
    "xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                 alpha):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=100, nfold=5,\n",
    "             seed=random_state,\n",
    "             callbacks=[xgb.callback.early_stop(5)])\n",
    "\n",
    "    return -cv_result['test-mae-mean'].values[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_state = SEED\n",
    "num_iter = 50\n",
    "init_points = 25\n",
    "params = {\n",
    "    'eta': 0.1,\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mae',\n",
    "    'verbose_eval': True,\n",
    "    'seed': random_state\n",
    "}\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 20),\n",
    "                                            'colsample_bytree': (0.1, 1),\n",
    "                                            'max_depth': (5, 20),\n",
    "                                            'subsample': (0.5, 1),\n",
    "                                            'gamma': (0, 15),\n",
    "                                            'alpha': (0, 15),\n",
    "                                            })\n",
    "\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "\n",
    "print(xgbBO.res['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train_df = process_df(shuffled_data,1,train = True)\n",
    "\n",
    "X_train = f_train_df.drop(labels=[\"Poder_Adquisitivo\"],axis=1).as_matrix()\n",
    "y_train = f_train_df[\"Poder_Adquisitivo\"].as_matrix()\n",
    "\n",
    "xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                 alpha):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=100, nfold=5,\n",
    "             seed=random_state,\n",
    "             callbacks=[xgb.callback.early_stop(5)])\n",
    "\n",
    "    return -cv_result['test-mae-mean'].values[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_state = SEED\n",
    "num_iter = 50\n",
    "init_points = 25\n",
    "params = {\n",
    "    'eta': 0.1,\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mae',\n",
    "    'verbose_eval': True,\n",
    "    'seed': random_state\n",
    "}\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 20),\n",
    "                                            'colsample_bytree': (0.1, 1),\n",
    "                                            'max_depth': (5, 20),\n",
    "                                            'subsample': (0.5, 1),\n",
    "                                            'gamma': (0, 15),\n",
    "                                            'alpha': (0, 15),\n",
    "                                            })\n",
    "\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "\n",
    "print(xgbBO.res['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train_df = process_df(shuffled_data,0,train = True)\n",
    "\n",
    "X_train = f_train_df.drop(labels=[\"Poder_Adquisitivo\"],axis=1).as_matrix()\n",
    "y_train = f_train_df[\"Poder_Adquisitivo\"].as_matrix()\n",
    "\n",
    "def model_evaluate(n_estimators,\n",
    "                 max_depth,\n",
    "                max_features,\n",
    "                 min_samples_leaf,\n",
    "                  ):\n",
    "\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    min_samples_leaf = int(min_samples_leaf)\n",
    "    max_features = max(min(max_features, 1), 0.1)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                 max_depth=max_depth,\n",
    "                max_features=max_features,\n",
    "                 min_samples_leaf=min_samples_leaf,\n",
    "                                 n_jobs=4,random_state=SEED)\n",
    "\n",
    "    cv_result = cross_val_score(model,X_train,y_train,cv=kf,scoring=\"neg_mean_absolute_error\")\n",
    "\n",
    "    return np.mean(cv_result)\n",
    "\n",
    "\n",
    "num_iter = 50\n",
    "init_points = 15\n",
    "\n",
    "\n",
    "modelBO = BayesianOptimization(model_evaluate, {'n_estimators': (30, 100),\n",
    "                                            'max_depth': (15, 25),\n",
    "                                            'min_samples_leaf': (1, 200),\n",
    "                                                'max_features' :(0.1,1),\n",
    "                                            })\n",
    "\n",
    "modelBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "\n",
    "print(modelBO.res['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model = RandomForestRegressor(n_estimators=100,\n",
    "                 max_depth=25,\n",
    "                max_features=1.0,\n",
    "                 min_samples_leaf=31,\n",
    "                                 n_jobs=4,random_state=SEED)\n",
    "\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('bla',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "49 | 21m01s | -4381.25574 |     25.0000 |         1.0000 |            31.3392 |       100.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_train_df = process_df(shuffled_data,0,train = True)\n",
    "\n",
    "X_train = f_train_df.drop(labels=[\"Poder_Adquisitivo\"],axis=1).as_matrix()\n",
    "y_train = f_train_df[\"Poder_Adquisitivo\"].as_matrix()\n",
    "\n",
    "def model_evaluate(n_estimators,\n",
    "                 max_depth,\n",
    "                max_features,\n",
    "                 min_samples_leaf,\n",
    "                  ):\n",
    "\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    min_samples_leaf = int(min_samples_leaf)\n",
    "    max_features = max(min(max_features, 1), 0.1)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                 max_depth=max_depth,\n",
    "                max_features=max_features,\n",
    "                 min_samples_leaf=min_samples_leaf,\n",
    "                                 n_jobs=4,random_state=SEED)\n",
    "\n",
    "    cv_result = cross_val_score(model,X_train,y_train,cv=kf,scoring=\"neg_median_absolute_error\")\n",
    "\n",
    "    return np.mean(cv_result)\n",
    "\n",
    "\n",
    "num_iter = 50\n",
    "init_points = 10\n",
    "\n",
    "\n",
    "modelBO_2 = BayesianOptimization(model_evaluate, {'n_estimators': (30, 100),\n",
    "                                            'max_depth': (17, 27),\n",
    "                                            'min_samples_leaf': (5, 200),\n",
    "                                                'max_features' :(0.5,1),\n",
    "                                            })\n",
    "\n",
    "modelBO_2.maximize(init_points=init_points, n_iter=num_iter)\n",
    "\n",
    "print(modelBO_2.res['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model = RandomForestRegressor(n_estimators=100,\n",
    "                 max_depth=27,\n",
    "                max_features=1.0,\n",
    "                 min_samples_leaf=5,\n",
    "                                 n_jobs=4,random_state=SEED)\n",
    "\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('bla',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = []\n",
    "splits = get_splits(kf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgb.XGBRegressor(max_depth=15, learning_rate=0.1, alpha=10,n_estimators=30, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=1.0,colsample_bytree=1.0,min_child_weight=1.0,random_state=SEED )\n",
    "\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('bla',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgb.XGBRegressor(max_depth=20, learning_rate=0.1, alpha=0,n_estimators=30, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=1.0,colsample_bytree=1.0,min_child_weight=20.0,random_state=SEED )\n",
    "\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('bla',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "params =  {'max_depth': 9, 'eta': 0.01, 'silent': 1, 'objective': 'reg:linear', 'eval_metric':'rmse','seed':SEED}\n",
    "model = XGBoostModel(params,500)\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(model,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "record_scores('raw_XGB_d9_eta0.01_n500',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking ml-ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = []\n",
    "splits = get_splits(kf,0)\n",
    "\n",
    "ensemble = SuperLearner(verbose=2)\n",
    "\n",
    "n = 146\n",
    "\n",
    "ensemble.add([LinearRegression(),\n",
    "          DecisionTreeRegressor(min_samples_split=200, min_samples_leaf=50, random_state=SEED),\n",
    "          GradientBoostingRegressor(random_state=SEED,min_samples_leaf=5),\n",
    "          xgb.XGBRegressor(max_depth=15, learning_rate=0.1, alpha=10,n_estimators=30, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=1.0,colsample_bytree=1.0,min_child_weight=1.0,random_state=SEED),\n",
    "          RandomForestRegressor(n_estimators=50,\n",
    "                                       criterion='mse', max_depth=23, max_features='auto',n_jobs=4,\n",
    "                                       min_samples_leaf=5,random_state=SEED)],propagate_features=[i for i in range(n)])\n",
    "\n",
    "ensemble.add_meta(RandomForestRegressor(n_estimators=50,\n",
    "                                       criterion='mse', max_depth=17, max_features='auto',n_jobs=4,\n",
    "                                       min_samples_leaf=5,random_state=SEED))\n",
    "\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(ensemble,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('alpha_ensemble',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedEnsemble:\n",
    "    def __init__(self,models,weights):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        for model in self.models:\n",
    "            model.fit(X,y)\n",
    "            \n",
    "    def predict(self,X):\n",
    "        y = np.zeros((X.shape[0]))\n",
    "        for i in range(len(self.models)):\n",
    "            y += (self.models[i].predict(X) * self.weights[i])\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = WeightedEnsemble([LinearRegression(),DecisionTreeRegressor()],[0.5,0.5])\n",
    "\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(ensemble,splits)\n",
    "\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('alpha_ensemble',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ensemble = WeightedEnsemble([xgb.XGBRegressor(max_depth=15, learning_rate=0.1, alpha=10,n_estimators=30, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=1.0,colsample_bytree=1.0,min_child_weight=1.0,random_state=SEED),\n",
    "                            RandomForestRegressor(n_estimators=50,\n",
    "                                       criterion='mse', max_depth=23, max_features='auto',n_jobs=4,\n",
    "                                       min_samples_leaf=5,random_state=SEED)],\n",
    "                            [0.5,0.5])\n",
    "\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(ensemble,splits)\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('beta_ensemble',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ensemble = WeightedEnsemble([xgb.XGBRegressor(max_depth=20, learning_rate=0.1, alpha=0,n_estimators=30, silent=True, objective='reg:linear', \n",
    "                         n_jobs=4, subsample=1.0,colsample_bytree=1.0,min_child_weight=20.0,random_state=SEED ),\n",
    "                            RandomForestRegressor(n_estimators=50,\n",
    "                                       criterion='mse', max_depth=23, max_features='auto',n_jobs=4,\n",
    "                                       min_samples_leaf=5,random_state=SEED)],\n",
    "                            [0.5,0.5])\n",
    "\n",
    "scores_rmse,scores_mae,scores_mad = train_and_evaluate(ensemble,splits)\n",
    "\n",
    "print(\"RMSE: %f\" % np.mean(scores_rmse))\n",
    "print(\"MAE: %f\" % np.mean(scores_mae))\n",
    "print(\"MAD: %f\" % np.mean(scores_mad))\n",
    "\n",
    "record_scores('beta_ensemble',np.mean(scores_rmse),np.mean(scores_mae),np.mean(scores_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
